{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autograder_cmap.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shresht77/autograder-for-concept-maps/blob/main/autograder_cmap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBNksSB-MRrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04811210-8d65-48b0-acc2-b0d6a4c2dff2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aXFWA21NQws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cb497e-e674-4c57-d358-a81bc3e776ad"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoFc6wuKCw9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c98e0f1-0126-4bf5-ad86-dc9035d2f36c"
      },
      "source": [
        "!pip install rake-nltk\n",
        "from rake_nltk import Rake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka3NxBcPMYYE",
        "outputId": "7c6b3e1f-82f2-4d5c-e671-8236dfed3ef2"
      },
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-z9mksef6\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-z9mksef6\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (0.8.7)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (1.19.5)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (1.5.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (2.5)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (0.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segtok->yake==0.4.3) (2019.12.20)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->yake==0.4.3) (4.4.2)\n",
            "Building wheels for collected packages: yake\n",
            "  Building wheel for yake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yake: filename=yake-0.4.3-py2.py3-none-any.whl size=66270 sha256=e239336b2da6141759644dbb9e9dd5ab64456a8dde24c2cad82507c08b056768\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7nk_zz71/wheels/be/35/27/e4ebd54b78c1806ed8b0271ce247fcd91e2bedde35889fbc9b\n",
            "Successfully built yake\n",
            "Installing collected packages: yake\n",
            "  Found existing installation: yake 0.3.7\n",
            "    Uninstalling yake-0.3.7:\n",
            "      Successfully uninstalled yake-0.3.7\n",
            "Successfully installed yake-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWX5_3iM_-s"
      },
      "source": [
        "import yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnhVLIhSC_vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836038a4-e48e-4551-85d2-18462692247f"
      },
      "source": [
        "kw_extractor = yake.KeywordExtractor()\n",
        "a= 'Sleuthkit.txt'\n",
        "out=a.replace(\".txt\",\"\")\n",
        "out = out+\".csv\"\n",
        "print(out)\n",
        "with open(a, 'r') as file:\n",
        "    data = file.read().replace('\\n', ' ')\n",
        "\n",
        "# Extraction given the text.\n",
        "one = \"\"\n",
        "keywords = kw_extractor.extract_keywords(data)\n",
        "print(keywords)\n",
        "for i in keywords:\n",
        "  a=i[0]\n",
        "  print (a)\n",
        "  one = one+\" \"+a\n",
        "\n",
        "print (one)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sleuthkit.csv\n",
            "[('brian carrier prefix', 0.00022543485334123927), ('block metadata filename', 0.0003995285111943949), ('calc function lists', 0.0005041942626442075), ('sleuthkit mac tools', 0.0005115847313892579), ('carrier prefix blk', 0.0005331854121465385), ('function lists information', 0.0006775210524564474), ('display allocation status', 0.0007526235564786425), ('metadata filename filesystem', 0.0007808867968602771), ('layer block metadata', 0.0008638327062820957), ('show deleted files', 0.0008792036581700128), ('layer displays content', 0.0009165050159590304), ('sleuthkit mac sleuthkit', 0.0009594581722767513), ('show file slack', 0.0010119577939000264), ('show undeleted files', 0.0010119577939000264), ('mac sleuthkit mac', 0.0011631583944490796), ('filename filesystem sleuthkit', 0.0011896957723282896), ('find calc function', 0.0012086455530347883), ('layer displays details', 0.0014771120927738583), ('ffind fsstat sleuthkit', 0.001664991779341119), ('sleuthkit mac', 0.002569538859600434)]\n",
            "brian carrier prefix\n",
            "block metadata filename\n",
            "calc function lists\n",
            "sleuthkit mac tools\n",
            "carrier prefix blk\n",
            "function lists information\n",
            "display allocation status\n",
            "metadata filename filesystem\n",
            "layer block metadata\n",
            "show deleted files\n",
            "layer displays content\n",
            "sleuthkit mac sleuthkit\n",
            "show file slack\n",
            "show undeleted files\n",
            "mac sleuthkit mac\n",
            "filename filesystem sleuthkit\n",
            "find calc function\n",
            "layer displays details\n",
            "ffind fsstat sleuthkit\n",
            "sleuthkit mac\n",
            " brian carrier prefix block metadata filename calc function lists sleuthkit mac tools carrier prefix blk function lists information display allocation status metadata filename filesystem layer block metadata show deleted files layer displays content sleuthkit mac sleuthkit show file slack show undeleted files mac sleuthkit mac filename filesystem sleuthkit find calc function layer displays details ffind fsstat sleuthkit sleuthkit mac\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kzBYm3GAgIr",
        "outputId": "638d3b0a-0d1f-408d-a2ac-017ece7bcb20"
      },
      "source": [
        "r = Rake()\n",
        "a= 'Sleuthkit.txt'\n",
        "out=a.replace(\".txt\",\"\")\n",
        "out = out+\".csv\"\n",
        "print(out)\n",
        "with open(a, 'r') as file:\n",
        "    data = file.read().replace('\\n', ' ')\n",
        "\n",
        "# Extraction given the text.\n",
        "r.extract_keywords_from_text(data)\n",
        "rank = r.get_ranked_phrases()\n",
        "print(rank)\n",
        "rank_res = \" \".join(rank)\n",
        "print(rank_res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sleuthkit.csv\n",
            "['suffix ls cat stat find calc function lists information', 'g ., find inode number given filename fls –', '“ inode ” 887042 ifind – n boudin', 'f fs layer block metadata filename filesystem sleuthkit', 'dd show undeleted files fls – f 128mb', 'dd show deleted files fls – u 128mb', 'sleuthkit mac sleuthkit mac tools “ light', 'ffind fsstat sleuthkit examples fsstat 128mb', '“ inode ” 136140 examples', 'mactime – h – b body', 'dd 136140 display contents associated', 'access date sleuthkit mac tools', 'dd 4000 display allocation status', 'creation dates … good news', 'g ., inodes allocated', 'mactime_output sleuthkit mac tools', 'dd 136140 display status', '“ free ” inodes', 'data layer mac information', 'interest … … traces', 'block 4000 ils –', 'dd show file slack', 'brian carrier prefix blk', 'sleuthkit http :// www', 'dd 4000 dump block', 'files fls –', 'thumbdrive image containing 128mb', 'mac information associated', '“ modification ”', 'filesystem ” idea', 'ifind fls', 'http :// wiki', 'dd list allocated', 'subsequent file access', 'dd show', 'layer displays details', 'layer displays content', 'deleted files', '• istat 128mb', '• icat 128mb', 'allocated blocks (-', 'illustrating modification date', 'layer sleuthkit', 'ils –', '4000 examples', 'timeline mactime output', 'given object', 'blkls –', 'dd dump', 'b body', 'files (-', 'filesystem info', 'dd view', 'r 128mb', 'html 128mb', 'deletion dates', 'layer maps', 'layer calculates', 'unallocated inodes', 'blkstat 128mb', 'blkcat 128mb', 'fls', 'retrieved mactime', 'fat image', '== allocated', 'sleuthkit', 'blkcalc ils', 'meta data', 'first file', 'file system', 'file name', 'file counts', 'block', 'timeline end', 'slide set', 'nifty things', 'many uses', 'including fat', 'files', 'directories etc', 'dd', '== slackspace', '(+ lots', '128mb', 'h', 'mactime', 'date', 'creation', 'body', 'modification', 'timeline', 'istat', 'icat', 'meta', 'blkstat', 'blkls', 'blkcat', 'un', 'title', 'time', 'something', 'since', 'php', 'org', 'often', 'layers', 'index', 'freed', 'e', 'detail', 'described', 'deleting', 'create', 'copy', 'begins', '3', '2']\n",
            "suffix ls cat stat find calc function lists information g ., find inode number given filename fls – “ inode ” 887042 ifind – n boudin f fs layer block metadata filename filesystem sleuthkit dd show undeleted files fls – f 128mb dd show deleted files fls – u 128mb sleuthkit mac sleuthkit mac tools “ light ffind fsstat sleuthkit examples fsstat 128mb “ inode ” 136140 examples mactime – h – b body dd 136140 display contents associated access date sleuthkit mac tools dd 4000 display allocation status creation dates … good news g ., inodes allocated mactime_output sleuthkit mac tools dd 136140 display status “ free ” inodes data layer mac information interest … … traces block 4000 ils – dd show file slack brian carrier prefix blk sleuthkit http :// www dd 4000 dump block files fls – thumbdrive image containing 128mb mac information associated “ modification ” filesystem ” idea ifind fls http :// wiki dd list allocated subsequent file access dd show layer displays details layer displays content deleted files • istat 128mb • icat 128mb allocated blocks (- illustrating modification date layer sleuthkit ils – 4000 examples timeline mactime output given object blkls – dd dump b body files (- filesystem info dd view r 128mb html 128mb deletion dates layer maps layer calculates unallocated inodes blkstat 128mb blkcat 128mb fls retrieved mactime fat image == allocated sleuthkit blkcalc ils meta data first file file system file name file counts block timeline end slide set nifty things many uses including fat files directories etc dd == slackspace (+ lots 128mb h mactime date creation body modification timeline istat icat meta blkstat blkls blkcat un title time something since php org often layers index freed e detail described deleting create copy begins 3 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Y_eIEX3oEV"
      },
      "source": [
        "import csv\n",
        "with open(out, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows([rank_res])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82V871qCZ8BE"
      },
      "source": [
        "# Wordnet implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biu1mIDPDLzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6209fcaf-3584-4842-c643-4ffcb44be4b4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfMrpyqDZLW"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from xml.dom import minidom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrK4VcuDbOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eba8d4f-f4ca-4857-b785-6c5245d14dbf"
      },
      "source": [
        "fil1 = 'Sleuthkit.cxl'\n",
        "xmldoc = minidom.parse(fil1)\n",
        "out_fi = fil1.replace(\".cxl\",\"\")\n",
        "out_fil = \"/content/drive/My Drive/Auto_waterloo/res/try/\" + out_fi + \"_link.csv\"\n",
        "out_fi = \"/content/drive/My Drive/Auto_waterloo/res/try/\" + out_fi + \"_nodelink.csv\"\n",
        "conceptlist = xmldoc.getElementsByTagName('concept')\n",
        "linklist = xmldoc.getElementsByTagName('linking-phrase')\n",
        "conlist = xmldoc.getElementsByTagName('connection')\n",
        "\n",
        "concept={}\n",
        "link={}\n",
        "connection={}\n",
        "connection_id={}\n",
        "connection_from={}\n",
        "connection_to={}\n",
        "final={}\n",
        "final_f=collections.defaultdict(list)\n",
        "no=[]\n",
        "\n",
        "l_len=len(linklist)\n",
        "c_len=len(conceptlist)\n",
        "co_len=len(conlist)\n",
        "link_master = []\n",
        "label_master = []\n",
        "final_connect = []\n",
        "\n",
        "a=0\n",
        "while a<l_len:\n",
        "  link[linklist[a].attributes['id'].value]=linklist[a].attributes['label'].value\n",
        "  link_master.append(linklist[a].attributes['label'].value)\n",
        "  a = a+1\n",
        "  link = { k:v.replace(\"\\n\",\" \") for k, v in link.items()}\n",
        "  link = { k:v.replace(\"\\n\",\" \") for k, v in link.items()}\n",
        "  link = { k:v.replace(\"  \",\" \") for k, v in link.items()}\n",
        "  link_master = [item.replace(\"\\n\", \" \") for item in link_master]\n",
        "  link_master = [item.replace(\"\\r\", \" \") for item in link_master]\n",
        "  link_master = [item.replace(\"  \", \" \") for item in link_master]\n",
        "\n",
        "a=0\n",
        "while a<c_len:\n",
        "  concept[conceptlist[a].attributes['id'].value] = conceptlist[a].attributes['label'].value\n",
        "  label_master.append(conceptlist[a].attributes['label'].value)\n",
        "  a = a+1\n",
        "  concept = { k:v.replace(\"\\n\",\" \") for k, v in concept.items()}\n",
        "  concept = { k:v.replace(\"\\r\",\" \") for k, v in concept.items()}\n",
        "  concept = { k:v.replace(\"  \",\" \") for k, v in concept.items()}\n",
        "  label_master = [item.replace(\"\\n\", \" \") for item in label_master]\n",
        "  label_master = [item.replace(\"\\r\", \" \") for item in label_master]\n",
        "  label_master = [item.replace(\"  \", \" \") for item in label_master]\n",
        "BON_t = \" \".join(label_master)\n",
        "\n",
        "a=0\n",
        "while a<co_len:\n",
        "  connection[str(a)] = [conlist[a].attributes['from-id'].value,conlist[a].attributes['to-id'].value]\n",
        "  if connection[str(a)][0] in concept:\n",
        "    connection[str(a)][0] = concept[connection[str(a)][0]]\n",
        "  elif connection[str(a)][0] in link:\n",
        "    connection[str(a)][0] = link[connection[str(a)][0]]\n",
        "  if connection[str(a)][1] in concept:\n",
        "    connection[str(a)][1] = concept[connection[str(a)][1]]\n",
        "  elif connection[str(a)][1] in link:\n",
        "    connection[str(a)][1] = link[connection[str(a)][1]]\n",
        "  connect1 = connection\n",
        "  if(a>0):\n",
        "    i=0\n",
        "    while i<a:\n",
        "      if(connection[str(a)][0] == connection[str(i)][0]):\n",
        "        connection[str(i)].append(connection[str(a)][1])\n",
        "        connection[str(a)][0] = \" \"\n",
        "        connection[str(a)][1] = \" \"\n",
        "      \n",
        "      i=i+1\n",
        "  a=a+1\n",
        "a=0\n",
        "i=0\n",
        "while a<co_len:\n",
        "  if(connection[str(a)][0]!=\" \"):\n",
        "    final[i]=connection[str(a)]\n",
        "    i=i+1\n",
        "  a=a+1\n",
        "\n",
        "\n",
        "a=0\n",
        "i=0\n",
        "while a<co_len:\n",
        "  if(connection[str(a)][0] in link_master):\n",
        "    j=0\n",
        "    for i in connection[str(a)]:\n",
        "      if(j==0):\n",
        "        ans=i\n",
        "        ans=ans.replace(\"\\r\",\"\")\n",
        "        \n",
        "      else:\n",
        "        final_f[ans].append(i)\n",
        "\n",
        "      j=j+1\n",
        "  a=a+1\n",
        "\n",
        "print(final_f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'list'>, {'prefix combined with suffix examples': ['icat : Display contents associated with \"inode\"', 'blkls: Displays data blocks within a file system ', 'ils : Lists all metadata entries, such as an Inode.', 'ffind searches for file names that point to a specified metadata entry', 'fls: Lists allocated and unallocated file names within a file system.', 'fsstat: Displays file system statistical information about an image or storage medium.'], 'Few other examples of command line tools': ['fsstat IMAGE_NAME.dd view all filesystem information', 'fls -F IMAGE_NAME.dd show only files', 'fls -d IMAGE_NAME.dd show deleted files', 'blkls -s IMAGE_NAME.dd show file slack on all files', 'fls -u IMAGE_NAME.dd show undeleted files', 'istat IMAGENAME.dd INODE_NUM Display status of INODE', 'fls -D IMAGE_NAME.dd show only directories', 'blkls -a IMAGE_NAME.dd gives details of allocated blocks'], 'Library used for forensics analysis of computer originated by Brain Carrier': ['Can be used as library embedded within a separate digital forensic tool such as Autopsy ', 'MAC(Modification,Access, Creation time)', 'Can be used by using command line tools'], 'Command line Prefix': ['blk:block', 'i : metadata', 'f: Filename', 'fs: Filesystem'], 'gives MAC information of all files': ['Deletion of file counts as \"modification\" and often deleted files can be retrieved', ' Creates a timeline of all files based upon their MAC times.'], 'command line suffix': ['ls:Lists information in the layer', 'find: maps other layers to its layer', 'cat: Display content in the layer', 'stat : Display details about a given object in the layer', 'calc: calculates \"something\" in the layer']})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLn-GezDhor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992403ea-da9d-420e-d0c1-ef5f02d4d4ef"
      },
      "source": [
        "print(label_master)\n",
        "print(link_master)\n",
        "linklis=[]\n",
        "for ass in link_master:\n",
        "  answe = ass.replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
        "  linklis.append(answe)\n",
        "print(linklis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i : metadata', 'stat : Display details about a given object in the layer', ' Creates a timeline of all files based upon their MAC times.', 'blkls: Displays data blocks within a file system ', 'blk:block', 'ls:Lists information in the layer', 'fsstat IMAGE_NAME.dd view all filesystem information', 'fs: Filesystem', 'Deletion of file counts as \"modification\" and often deleted files can be retrieved', 'icat : Display contents associated with \"inode\"', 'calc: calculates \"something\" in the layer', 'istat IMAGENAME.dd INODE_NUM Display status of INODE', 'fls -u IMAGE_NAME.dd show undeleted files', 'ffind searches for file names that point to a specified metadata entry', 'cat: Display content in the layer', 'Can be used as library embedded within a separate digital forensic tool such as Autopsy ', 'f: Filename', 'SleuthKit', 'fls -d IMAGE_NAME.dd show deleted files', 'fls -D IMAGE_NAME.dd show only directories', 'fls -F IMAGE_NAME.dd show only files', 'blkls -a IMAGE_NAME.dd gives details of allocated blocks', 'find: maps other layers to its layer', 'fsstat: Displays file system statistical information about an image or storage medium.', 'blkls -s IMAGE_NAME.dd show file slack on all files', 'fls: Lists allocated and unallocated file names within a file system.', 'Can be used by using command line tools', 'MAC(Modification,Access, Creation time)', 'ils : Lists all metadata entries, such as an Inode.', 'Forensics Investigation Tool']\n",
            "['command line suffix', 'Few other examples of command line tools', 'gives MAC information of all files', 'Command line Prefix', 'Library used for forensics analysis of computer originated by Brain Carrier', 'prefix combined with suffix examples']\n",
            "['command line suffix', 'Few other examples of command line tools', 'gives MAC information of all files', 'Command line Prefix', 'Library used for forensics analysis of computer originated by Brain Carrier', 'prefix combined with suffix examples']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkMSni6zDkNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed3f4bd-d1b9-4f20-9649-fc3878149033"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import FreqDist\n",
        "import pandas as pd\n",
        "from csv import reader\n",
        " \n",
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        "def tagged_to_synset(word, tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag is None:\n",
        "        return None\n",
        " \n",
        "    try:\n",
        "        return wn.synsets(word, wn_tag)[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
        "    # Tokenize and tag\n",
        "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
        "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
        " \n",
        "    # Get the synsets for the tagged words\n",
        "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
        "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
        "    \n",
        "    # Filter out the Nones\n",
        "    synsets1 = [ss for ss in synsets1 if ss]\n",
        "    synsets2 = [ss for ss in synsets2 if ss]\n",
        "\n",
        "    score, count = 0.0, 0\n",
        " \n",
        "    # For each word in the first sentence\n",
        "    for synset in synsets1:\n",
        "        simlist = [synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None]\n",
        "        if not simlist:\n",
        "            continue;\n",
        "        best_score = max(simlist)\n",
        " \n",
        "        # Check that the similarity could have been computed\n",
        "        score += best_score\n",
        "        count += 1\n",
        "\n",
        " \n",
        "    # Average the values\n",
        "    try:\n",
        "      score /= count\n",
        "    except Exception as e:\n",
        "      score = 1.0\n",
        "    return score\n",
        " \n",
        "sentences = [\n",
        "\n",
        "]\n",
        "\n",
        "focus_sentence = rank_res\n",
        "#link-node pair\n",
        "for a in final_f:\n",
        "  x=\"\"\n",
        "  x=a\n",
        "  for b in final_f[a]:\n",
        "    x = x+\" \"+b\n",
        "  sentences.append(x)\n",
        "CSVStudentAnswer = \" \".join(sentences)\n",
        "sum = 0\n",
        "j=0\n",
        "count = 0\n",
        "res1=[]\n",
        "for sentence in sentences:\n",
        "    sen1 = sentence.lower()\n",
        "    sim_res = sentence_similarity(sen1, focus_sentence)*100\n",
        "    res1.append(sim_res)\n",
        "    print (\"Similarity(\\\"%s\\\") = %s\" % (sentence, sim_res))\n",
        "    \n",
        "\n",
        "    if sim_res > 50:\n",
        "      count = count+1\n",
        "    j=j+1\n",
        "sim_r=\"\"\n",
        "similarity_result= (count/j)*100 \n",
        "if similarity_result>75:\n",
        "  sim_r=\"Excellent\"\n",
        "elif similarity_result>50:\n",
        "  sim_r=\"Good\"\n",
        "elif similarity_result>25:\n",
        "  sim_r=\"Poor\"\n",
        "else:\n",
        "  sim_r=\"Failing\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity(\"prefix combined with suffix examples icat : Display contents associated with \"inode\" blkls: Displays data blocks within a file system  ils : Lists all metadata entries, such as an Inode. ffind searches for file names that point to a specified metadata entry fls: Lists allocated and unallocated file names within a file system. fsstat: Displays file system statistical information about an image or storage medium.\") = 78.21428571428572\n",
            "Similarity(\"Few other examples of command line tools fsstat IMAGE_NAME.dd view all filesystem information fls -F IMAGE_NAME.dd show only files fls -d IMAGE_NAME.dd show deleted files blkls -s IMAGE_NAME.dd show file slack on all files fls -u IMAGE_NAME.dd show undeleted files istat IMAGENAME.dd INODE_NUM Display status of INODE fls -D IMAGE_NAME.dd show only directories blkls -a IMAGE_NAME.dd gives details of allocated blocks\") = 93.19444444444444\n",
            "Similarity(\"Library used for forensics analysis of computer originated by Brain Carrier Can be used as library embedded within a separate digital forensic tool such as Autopsy  MAC(Modification,Access, Creation time) Can be used by using command line tools\") = 47.763636363636365\n",
            "Similarity(\"Command line Prefix blk:block i : metadata f: Filename fs: Filesystem\") = 71.25\n",
            "Similarity(\"gives MAC information of all files Deletion of file counts as \"modification\" and often deleted files can be retrieved  Creates a timeline of all files based upon their MAC times.\") = 78.15126050420169\n",
            "Similarity(\"command line suffix ls:Lists information in the layer find: maps other layers to its layer cat: Display content in the layer stat : Display details about a given object in the layer calc: calculates \"something\" in the layer\") = 83.63888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ5H9usY_X-j"
      },
      "source": [
        "a = {'Sentence': sentences, 'Wordnet similarity': res1}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(out_fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzBeF31tEMiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd1ae3c-0bc8-4fc9-f16f-2c08c4812c94"
      },
      "source": [
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        "def tagged_to_synset(word, tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag is None:\n",
        "        return None\n",
        " \n",
        "    try:\n",
        "        return wn.synsets(word, wn_tag)[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
        "    # Tokenize and tag\n",
        "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
        "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
        " \n",
        "    # Get the synsets for the tagged words\n",
        "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
        "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
        "    \n",
        "    # Filter out the Nones\n",
        "    synsets1 = [ss for ss in synsets1 if ss]\n",
        "    synsets2 = [ss for ss in synsets2 if ss]\n",
        "\n",
        "    score, count = 0.0, 0\n",
        " \n",
        "    # For each word in the first sentence\n",
        "    for synset in synsets1:\n",
        "        simlist = [synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None]\n",
        "        if not simlist:\n",
        "            continue;\n",
        "        best_score = max(simlist)\n",
        " \n",
        "        # Check that the similarity could have been computed\n",
        "        score += best_score\n",
        "        count += 1\n",
        "\n",
        " \n",
        "    # Average the values\n",
        "    try:\n",
        "      score /= count\n",
        "    except Exception as e:\n",
        "      score = 0.0\n",
        "    return score\n",
        " \n",
        "sentences = [\n",
        "\n",
        "]\n",
        "\n",
        "focus_sentence = rank_res\n",
        "#link-node pair\n",
        "for a in label_master:\n",
        "  x=\"\"\n",
        "  x=a.replace(\"/\",\" or \").replace(\"  \",\" \")\n",
        "  sentences.append(x)\n",
        "CSVStudentAnswer = \" \".join(sentences)\n",
        "sum = 0\n",
        "j=0\n",
        "count = 0\n",
        "\n",
        "res1=[]\n",
        "for sentence in sentences:\n",
        "    sen1 = sentence.lower()\n",
        "    sim_res = sentence_similarity(sen1, focus_sentence)*100\n",
        "    res1.append(sim_res)\n",
        "    print (\"Similarity(\\\"%s\\\") = %s\" % (sentence, sim_res))\n",
        "    if sim_res > 50:\n",
        "      count = count+1\n",
        "    j=j+1\n",
        "\n",
        "bof_similarity_result= (count/j)*100\n",
        "bof_similarity_result=round(bof_similarity_result)\n",
        "print(bof_similarity_result)\n",
        "bof_res=\"\"\n",
        "if bof_similarity_result>75:\n",
        "  bof_res=\"Excellent\"\n",
        "elif bof_similarity_result>50:\n",
        "  bof_res=\"Good\"\n",
        "elif bof_similarity_result>25:\n",
        "  bof_res=\"Poor\"\n",
        "else:\n",
        "  bof_res=\"Failing\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity(\"6-Primary Bit 1 if its sender 0 if its receiver\") = 9.971509971509972\n",
            "Similarity(\"2) Length\") = 12.5\n",
            "Similarity(\"Cyclic Redundancy Check Code (CRC)\") = 9.114959114959115\n",
            "Similarity(\"Data Block\") = 11.11111111111111\n",
            "Similarity(\"7-Direction Bit\") = 11.11111111111111\n",
            "Similarity(\"Sender 4-Frame Count Valid Bit 5-Frame Count Bit\") = 12.380952380952381\n",
            "Similarity(\"3) Control\") = 10.0\n",
            "Similarity(\"5) Source Address\") = 9.722222222222221\n",
            "Similarity(\"DATA LINK LAYER\") = 11.11111111111111\n",
            "Similarity(\"Address of Receiver\") = 8.012820512820513\n",
            "Similarity(\"0-3 Function Code\") = 12.698412698412698\n",
            "Similarity(\"CRC-32 Bytes\") = 16.666666666666664\n",
            "Similarity(\"Header-10 Bytes\") = 16.666666666666664\n",
            "Similarity(\"4) Destination Address\") = 8.712121212121213\n",
            "Similarity(\"User Data-250 Bytes\") = 16.666666666666664\n",
            "Similarity(\"Address of Sender\") = 9.722222222222221\n",
            "Similarity(\"Receiver 4-DFC 5-RES\") = 0.0\n",
            "Similarity(\"DATA LINK LAYER FRAME FORMAT\") = 10.333333333333332\n",
            "Similarity(\"1) Start\") = 9.090909090909092\n",
            "Similarity(\"Count of User Data\") = 12.632275132275131\n",
            "Similarity(\"6) CRC\") = 0.0\n",
            "Similarity(\"Start of Data Block\") = 10.43771043771044\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUdzuyi9BqAO"
      },
      "source": [
        "a = {'Sentence': sentences, 'Wordnet similarity': res1}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(out_fil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6taoEuWGym"
      },
      "source": [
        "Embeddedness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGAOW6oEY18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ad7401-362b-4d5f-f35d-954d131c9ad5"
      },
      "source": [
        "embed_res=(l_len/(c_len-1))*100\n",
        "\n",
        "embed_res=round(embed_res)\n",
        "print(embed_res)\n",
        "embed_result =\"\"\n",
        "if embed_res>75:\n",
        "  embed_result = \"Excellent\"\n",
        "elif embed_res>50:\n",
        "  embed_result = \"Good\"\n",
        "elif embed_res>25:\n",
        "  embed_result = \"Poor\"\n",
        "else:\n",
        "  embed_result = \"Failing\"\n",
        "\n",
        "print(embed_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n",
            "Poor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcLo28pGEjvC"
      },
      "source": [
        "res_link=[]\n",
        "res_link1=[]\n",
        "for i in link_master:\n",
        "  link_res = i.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"  \",\" \")\n",
        "  \n",
        "  res_link.append(link_res)\n",
        "\n",
        "all_link = len(res_link)\n",
        "\n",
        "uniq_link= len(Counter(res_link).keys())\n",
        "efflink_result=\"\"\n",
        "efflink= (uniq_link/all_link)*100\n",
        "efflink=round(efflink)\n",
        "if efflink == 100:\n",
        "  efflink_result=\"Excellent\"\n",
        "elif efflink > 75:\n",
        "  efflink_result=\"Good\"\n",
        "else:\n",
        "  cu = []\n",
        "  su=0\n",
        "  #can use this itself for efflink\n",
        "  for a in final_f:\n",
        "    cu.append(a)\n",
        "  res_effi =[]\n",
        "  for a in final_f:\n",
        "    j=\"\"\n",
        "    cu.remove(a)\n",
        "    j = \" \".join(cu)\n",
        "    resu = symmetric_sentence_similarity(a, j)*100\n",
        "    res_effi.append(resu)\n",
        "    cu.append(a)\n",
        "\n",
        "  for n in res_effi:\n",
        "    su = su + n\n",
        "\n",
        "  link_sim=su/len(res_effi)\n",
        "  if link_sim<50:\n",
        "    efflink_result=\"Poor\"\n",
        "  else:\n",
        "    efflink_result=\"Failing\"\n",
        "\n",
        "print(efflink_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeMtGRpEmrN"
      },
      "source": [
        "embed=collections.defaultdict(list)\n",
        "for i in final:\n",
        "  k=0\n",
        "  for j in final[i]:\n",
        "    if(k==0):\n",
        "      ans=j\n",
        "      ans=ans.replace(\"\\r\",\"\").replace(\"  \",\" \")\n",
        "\n",
        "    else:\n",
        "      j=j.replace(\"\\r\",\"\").replace(\"  \",\" \")\n",
        "      embed[ans].append(j)\n",
        "\n",
        "    k=k+1\n",
        "print (embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjHJymO-Ewt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15776c8e-2227-45ab-a380-816fe32f2e59"
      },
      "source": [
        "embed_link=collections.defaultdict(list)\n",
        "links=[]\n",
        "node_list=[]\n",
        "for i in label_master:\n",
        "  node_list.append(i)\n",
        "for i in link_master:\n",
        "  ans12=i.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
        "  links.append(ans12)\n",
        "for i in embed:\n",
        "  if i in links:\n",
        "    for j in embed[i]:\n",
        "      embed_link[i].append(j)\n",
        "\n",
        "for i in embed_link:\n",
        "  for j in embed_link[i]:\n",
        "    if j in node_list:\n",
        "      node_list.remove(j)\n",
        "    \n",
        "print (node_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DATA LINK LAYER', 'DATA LINK LAYER FRAME FORMAT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKDyyxilEzVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34031df-4890-4c97-d747-226fac612ab8"
      },
      "source": [
        "print(final)\n",
        "full=[]\n",
        "for i in final:\n",
        "  for j in final[i]:\n",
        "    aswe=j.replace(\"  \",\" \")\n",
        "    full.append(aswe)\n",
        "print(full)\n",
        "print(node_list)\n",
        "for i in node_list:\n",
        "  if i not in full:\n",
        "    node_list.remove(i)\n",
        "\n",
        "\n",
        "rootnode=node_list[0]\n",
        "rootnode=rootnode.replace(\"  \",\" \")\n",
        "print(node_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ['5) Source Address', '2 Bytes represents'], 1: ['Frame structure contains ', 'Data Block', 'Header-10 Bytes'], 2: ['4) Destination Address', '2 Bytes represents'], 3: ['2 Bytes represents', 'Address of Receiver', 'Address of Sender', 'Start of Data Block', 'Cyclic Redundancy Check Code (CRC)'], 4: ['is comprised with following bytes', '2) Length', '4) Destination Address', '6) CRC', '3) Control', '5) Source Address', '1) Start'], 5: ['1 Byte represents', '6-Primary Bit 1 if its sender 0 if its receiver', 'Count of User Data', 'Sender 4-Frame Count Valid Bit 5-Frame Count Bit', 'Receiver 4-DFC 5-RES', '0-3 Function Code', '7-Direction Bit'], 6: ['DATA LINK LAYER', 'Frame structure contains '], 7: ['Header-10 Bytes', 'is comprised with following bytes'], 8: ['Data Block', 'structure combined with'], 9: ['2) Length', '1 Byte represents'], 10: ['structure combined with', 'User Data-250 Bytes', 'CRC-32 Bytes'], 11: ['3) Control', '1 Byte represents'], 12: ['6) CRC', '2 Bytes represents'], 13: ['1) Start', '2 Bytes represents']}\n",
            "['5) Source Address', '2 Bytes represents', 'Frame structure contains ', 'Data Block', 'Header-10 Bytes', '4) Destination Address', '2 Bytes represents', '2 Bytes represents', 'Address of Receiver', 'Address of Sender', 'Start of Data Block', 'Cyclic Redundancy Check Code (CRC)', 'is comprised with following bytes', '2) Length', '4) Destination Address', '6) CRC', '3) Control', '5) Source Address', '1) Start', '1 Byte represents', '6-Primary Bit 1 if its sender 0 if its receiver', 'Count of User Data', 'Sender 4-Frame Count Valid Bit 5-Frame Count Bit', 'Receiver 4-DFC 5-RES', '0-3 Function Code', '7-Direction Bit', 'DATA LINK LAYER', 'Frame structure contains ', 'Header-10 Bytes', 'is comprised with following bytes', 'Data Block', 'structure combined with', '2) Length', '1 Byte represents', 'structure combined with', 'User Data-250 Bytes', 'CRC-32 Bytes', '3) Control', '1 Byte represents', '6) CRC', '2 Bytes represents', '1) Start', '2 Bytes represents']\n",
            "['DATA LINK LAYER', 'DATA LINK LAYER FRAME FORMAT']\n",
            "['DATA LINK LAYER']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_-n2v4ZE5zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9fbdbb-6a53-4d22-ae7f-17026fd450f5"
      },
      "source": [
        "print(rootnode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA LINK LAYER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgGRagCi3LEC"
      },
      "source": [
        "#breadth if more than one rootnode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EwG526Z1lL"
      },
      "source": [
        "res_D = []\n",
        "if(len(node_list)>1):\n",
        "  for i in node_list:\n",
        "    rootnode = i\n",
        "    depth=0\n",
        "    dfs(visited, embed, rootnode, depth)\n",
        "    max_dep = 0\n",
        "    for i in depth_val:\n",
        "      if i > max_dep:\n",
        "        max_dep = i\n",
        "    result_depth=\"\"\n",
        "\n",
        "    depth_res=(max_dep-1)/2\n",
        "    depth_res=round(depth_res)\n",
        "    res_D.append(depth_res)\n",
        "    print(res_D)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ79YT9lbJ5L"
      },
      "source": [
        "depth_res=0\n",
        "for i in res_D:\n",
        "  if(i>depth_res):\n",
        "    depth_res = i\n",
        "print(depth_res)\n",
        "result_depth=sol_embed(depth_res)\n",
        "print(result_depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD4oejK33O62"
      },
      "source": [
        "#breadth if only one rootnode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90DgHsAJI5GR"
      },
      "source": [
        "visited = set()\n",
        "depth_val=[]\n",
        "def dfs(visited, graph, node, depth):\n",
        "    if node not in visited:\n",
        "        visited.add(node)\n",
        "        depth=depth+1\n",
        "        depth_val.append(depth)\n",
        "        for neighbour in graph[node]:\n",
        "            dfs(visited, graph, neighbour, depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlh9cXVcI8BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca234c6-94eb-4d9c-994f-bc7c58e624a6"
      },
      "source": [
        "depth=0\n",
        "\n",
        "dfs(visited, embed, rootnode, depth)\n",
        "print(depth_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 5, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 5, 6, 7, 7, 7, 7, 5, 5, 5, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie9VVDwYI-fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3c45b3-ec7d-42d7-cdb9-d0fa5b8dea10"
      },
      "source": [
        "max_dep = 0\n",
        "for i in depth_val:\n",
        "  if i > max_dep:\n",
        "    max_dep = i\n",
        "result_depth=\"\"\n",
        "\n",
        "depth_res=(max_dep-1)/2\n",
        "depth_res=round(depth_res)\n",
        "if depth_res==0:\n",
        "  result_depth = \"Failing\"\n",
        "elif depth_res == 1:\n",
        "  result_depth = \"Poor\"\n",
        "elif depth_res == 2:\n",
        "  result_depth = \"Good\"\n",
        "else:\n",
        "  result_depth = \"Excellent\"\n",
        "print(result_depth)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excellent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkLVBb8pUiIQ"
      },
      "source": [
        "def check(check1):\n",
        "  if check1 == \"Excellent\" :\n",
        "    return 3\n",
        "  elif check1 == \"Good\":\n",
        "    return 2\n",
        "  elif check1 == \"Poor\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjEBX4pJVOuP"
      },
      "source": [
        "def sol(val):\n",
        "  if val == 3:\n",
        "    return \"Excellent\"\n",
        "  elif val == 2:\n",
        "    return \"Good\"\n",
        "  elif val == 1:\n",
        "    return \"Poor\"\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj6aSGaxgAyW"
      },
      "source": [
        "def sol_embed(val):\n",
        "  if val > 3:\n",
        "    return \"Excellent\"\n",
        "  elif val == 2:\n",
        "    return \"Good\"\n",
        "  elif val == 1:\n",
        "    return \"Poor\"\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2en6Z1BVLQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f10ec32-4403-4e2d-c9c8-c9e822dc13eb"
      },
      "source": [
        "bn1=check(bof_res)\n",
        "\n",
        "bn2=check(result_depth)\n",
        "\n",
        "che=round((bn1+bn2)/2)\n",
        "print(che)\n",
        "bn3=sol(che)\n",
        "print(bn3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "Good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywSgv3W_WxfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19346ad6-6eb0-47b4-9cb3-515f8e3056cf"
      },
      "source": [
        "efl1=check(efflink_result)\n",
        "efl2=check(sim_r)\n",
        "che2 = round((efl1+efl2)/2)\n",
        "\n",
        "efl3=sol(che2)\n",
        "print(efl3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzFbPcOmJBFO"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from csv import reader\n",
        "\n",
        "emb=[]\n",
        "ef=[]\n",
        "re=[]\n",
        "bo=[]\n",
        "si=[]\n",
        "bof=[]\n",
        "#Embed\n",
        "emb.append(embed_result)\n",
        "#efficient link\n",
        "ef.append(efl3)\n",
        "#descriptive link\n",
        "si.append(sim_r)\n",
        "#BON\n",
        "re.append(result_depth)\n",
        "bo.append(bof_res)\n",
        "bof.append(bn3)\n",
        "out_file = fil1.replace(\".cxl\",\"\")\n",
        "full_final = '/content/drive/My Drive/Auto_waterloo/res/try/' + out_file +'_AutoWaterloo.csv'\n",
        "a = {'Breadth of net': bof,'Embeddedness': emb, \"Use of descriptive links\":si,'Efficient links': ef, 'Breadth of net_depth': depth_res, \"Breadth of net_Nodes similarity ratio\":bof_similarity_result, 'Embed value':embed_res, 'Use of descriptive link':similarity_result,'efficient_link val':efflink}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(full_final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI1WTj1-JC6X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}