{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autograder_cmap_no_output.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shresht77/autograder-for-concept-maps/blob/main/autograder_cmap_no_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBNksSB-MRrs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoFc6wuKCw9F"
      },
      "source": [
        "!pip install rake-nltk\n",
        "from rake_nltk import Rake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka3NxBcPMYYE"
      },
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWX5_3iM_-s"
      },
      "source": [
        "import yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnhVLIhSC_vw"
      },
      "source": [
        "kw_extractor = yake.KeywordExtractor()\n",
        "a= 'Sleuthkit.txt'\n",
        "out=a.replace(\".txt\",\"\")\n",
        "out = out+\".csv\"\n",
        "print(out)\n",
        "with open(a, 'r') as file:\n",
        "    data = file.read().replace('\\n', ' ')\n",
        "\n",
        "# Extraction given the text.\n",
        "one = \"\"\n",
        "keywords = kw_extractor.extract_keywords(data)\n",
        "print(keywords)\n",
        "for i in keywords:\n",
        "  a=i[0]\n",
        "  print (a)\n",
        "  one = one+\" \"+a\n",
        "\n",
        "print (one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kzBYm3GAgIr"
      },
      "source": [
        "r = Rake()\n",
        "a= 'Sleuthkit.txt'\n",
        "out=a.replace(\".txt\",\"\")\n",
        "out = out+\".csv\"\n",
        "print(out)\n",
        "with open(a, 'r') as file:\n",
        "    data = file.read().replace('\\n', ' ')\n",
        "\n",
        "# Extraction given the text.\n",
        "r.extract_keywords_from_text(data)\n",
        "rank = r.get_ranked_phrases()\n",
        "print(rank)\n",
        "rank_res = \" \".join(rank)\n",
        "print(rank_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Y_eIEX3oEV"
      },
      "source": [
        "import csv\n",
        "with open(out, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows([rank_res])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82V871qCZ8BE"
      },
      "source": [
        "# Wordnet implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biu1mIDPDLzT"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfMrpyqDZLW"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from xml.dom import minidom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrK4VcuDbOl"
      },
      "source": [
        "fil1 = 'Sleuthkit.cxl'\n",
        "xmldoc = minidom.parse(fil1)\n",
        "out_fi = fil1.replace(\".cxl\",\"\")\n",
        "out_fil = \"/content/drive/My Drive/Auto_waterloo/res/try/\" + out_fi + \"_link.csv\"\n",
        "out_fi = \"/content/drive/My Drive/Auto_waterloo/res/try/\" + out_fi + \"_nodelink.csv\"\n",
        "conceptlist = xmldoc.getElementsByTagName('concept')\n",
        "linklist = xmldoc.getElementsByTagName('linking-phrase')\n",
        "conlist = xmldoc.getElementsByTagName('connection')\n",
        "\n",
        "concept={}\n",
        "link={}\n",
        "connection={}\n",
        "connection_id={}\n",
        "connection_from={}\n",
        "connection_to={}\n",
        "final={}\n",
        "final_f=collections.defaultdict(list)\n",
        "no=[]\n",
        "\n",
        "l_len=len(linklist)\n",
        "c_len=len(conceptlist)\n",
        "co_len=len(conlist)\n",
        "link_master = []\n",
        "label_master = []\n",
        "final_connect = []\n",
        "\n",
        "a=0\n",
        "while a<l_len:\n",
        "  link[linklist[a].attributes['id'].value]=linklist[a].attributes['label'].value\n",
        "  link_master.append(linklist[a].attributes['label'].value)\n",
        "  a = a+1\n",
        "  link = { k:v.replace(\"\\n\",\" \") for k, v in link.items()}\n",
        "  link = { k:v.replace(\"\\n\",\" \") for k, v in link.items()}\n",
        "  link = { k:v.replace(\"  \",\" \") for k, v in link.items()}\n",
        "  link_master = [item.replace(\"\\n\", \" \") for item in link_master]\n",
        "  link_master = [item.replace(\"\\r\", \" \") for item in link_master]\n",
        "  link_master = [item.replace(\"  \", \" \") for item in link_master]\n",
        "\n",
        "a=0\n",
        "while a<c_len:\n",
        "  concept[conceptlist[a].attributes['id'].value] = conceptlist[a].attributes['label'].value\n",
        "  label_master.append(conceptlist[a].attributes['label'].value)\n",
        "  a = a+1\n",
        "  concept = { k:v.replace(\"\\n\",\" \") for k, v in concept.items()}\n",
        "  concept = { k:v.replace(\"\\r\",\" \") for k, v in concept.items()}\n",
        "  concept = { k:v.replace(\"  \",\" \") for k, v in concept.items()}\n",
        "  label_master = [item.replace(\"\\n\", \" \") for item in label_master]\n",
        "  label_master = [item.replace(\"\\r\", \" \") for item in label_master]\n",
        "  label_master = [item.replace(\"  \", \" \") for item in label_master]\n",
        "BON_t = \" \".join(label_master)\n",
        "\n",
        "a=0\n",
        "while a<co_len:\n",
        "  connection[str(a)] = [conlist[a].attributes['from-id'].value,conlist[a].attributes['to-id'].value]\n",
        "  if connection[str(a)][0] in concept:\n",
        "    connection[str(a)][0] = concept[connection[str(a)][0]]\n",
        "  elif connection[str(a)][0] in link:\n",
        "    connection[str(a)][0] = link[connection[str(a)][0]]\n",
        "  if connection[str(a)][1] in concept:\n",
        "    connection[str(a)][1] = concept[connection[str(a)][1]]\n",
        "  elif connection[str(a)][1] in link:\n",
        "    connection[str(a)][1] = link[connection[str(a)][1]]\n",
        "  connect1 = connection\n",
        "  if(a>0):\n",
        "    i=0\n",
        "    while i<a:\n",
        "      if(connection[str(a)][0] == connection[str(i)][0]):\n",
        "        connection[str(i)].append(connection[str(a)][1])\n",
        "        connection[str(a)][0] = \" \"\n",
        "        connection[str(a)][1] = \" \"\n",
        "      \n",
        "      i=i+1\n",
        "  a=a+1\n",
        "a=0\n",
        "i=0\n",
        "while a<co_len:\n",
        "  if(connection[str(a)][0]!=\" \"):\n",
        "    final[i]=connection[str(a)]\n",
        "    i=i+1\n",
        "  a=a+1\n",
        "\n",
        "\n",
        "a=0\n",
        "i=0\n",
        "while a<co_len:\n",
        "  if(connection[str(a)][0] in link_master):\n",
        "    j=0\n",
        "    for i in connection[str(a)]:\n",
        "      if(j==0):\n",
        "        ans=i\n",
        "        ans=ans.replace(\"\\r\",\"\")\n",
        "        \n",
        "      else:\n",
        "        final_f[ans].append(i)\n",
        "\n",
        "      j=j+1\n",
        "  a=a+1\n",
        "\n",
        "print(final_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLn-GezDhor"
      },
      "source": [
        "print(label_master)\n",
        "print(link_master)\n",
        "linklis=[]\n",
        "for ass in link_master:\n",
        "  answe = ass.replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
        "  linklis.append(answe)\n",
        "print(linklis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkMSni6zDkNI"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import FreqDist\n",
        "import pandas as pd\n",
        "from csv import reader\n",
        " \n",
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        "def tagged_to_synset(word, tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag is None:\n",
        "        return None\n",
        " \n",
        "    try:\n",
        "        return wn.synsets(word, wn_tag)[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
        "    # Tokenize and tag\n",
        "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
        "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
        " \n",
        "    # Get the synsets for the tagged words\n",
        "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
        "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
        "    \n",
        "    # Filter out the Nones\n",
        "    synsets1 = [ss for ss in synsets1 if ss]\n",
        "    synsets2 = [ss for ss in synsets2 if ss]\n",
        "\n",
        "    score, count = 0.0, 0\n",
        " \n",
        "    # For each word in the first sentence\n",
        "    for synset in synsets1:\n",
        "        simlist = [synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None]\n",
        "        if not simlist:\n",
        "            continue;\n",
        "        best_score = max(simlist)\n",
        " \n",
        "        # Check that the similarity could have been computed\n",
        "        score += best_score\n",
        "        count += 1\n",
        "\n",
        " \n",
        "    # Average the values\n",
        "    try:\n",
        "      score /= count\n",
        "    except Exception as e:\n",
        "      score = 1.0\n",
        "    return score\n",
        " \n",
        "sentences = [\n",
        "\n",
        "]\n",
        "\n",
        "focus_sentence = rank_res\n",
        "#link-node pair\n",
        "for a in final_f:\n",
        "  x=\"\"\n",
        "  x=a\n",
        "  for b in final_f[a]:\n",
        "    x = x+\" \"+b\n",
        "  sentences.append(x)\n",
        "CSVStudentAnswer = \" \".join(sentences)\n",
        "sum = 0\n",
        "j=0\n",
        "count = 0\n",
        "res1=[]\n",
        "for sentence in sentences:\n",
        "    sen1 = sentence.lower()\n",
        "    sim_res = sentence_similarity(sen1, focus_sentence)*100\n",
        "    res1.append(sim_res)\n",
        "    print (\"Similarity(\\\"%s\\\") = %s\" % (sentence, sim_res))\n",
        "    \n",
        "\n",
        "    if sim_res > 50:\n",
        "      count = count+1\n",
        "    j=j+1\n",
        "sim_r=\"\"\n",
        "similarity_result= (count/j)*100 \n",
        "if similarity_result>75:\n",
        "  sim_r=\"Excellent\"\n",
        "elif similarity_result>50:\n",
        "  sim_r=\"Good\"\n",
        "elif similarity_result>25:\n",
        "  sim_r=\"Poor\"\n",
        "else:\n",
        "  sim_r=\"Failing\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ5H9usY_X-j"
      },
      "source": [
        "a = {'Sentence': sentences, 'Wordnet similarity': res1}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(out_fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzBeF31tEMiI"
      },
      "source": [
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        "def tagged_to_synset(word, tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag is None:\n",
        "        return None\n",
        " \n",
        "    try:\n",
        "        return wn.synsets(word, wn_tag)[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
        "    # Tokenize and tag\n",
        "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
        "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
        " \n",
        "    # Get the synsets for the tagged words\n",
        "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
        "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
        "    \n",
        "    # Filter out the Nones\n",
        "    synsets1 = [ss for ss in synsets1 if ss]\n",
        "    synsets2 = [ss for ss in synsets2 if ss]\n",
        "\n",
        "    score, count = 0.0, 0\n",
        " \n",
        "    # For each word in the first sentence\n",
        "    for synset in synsets1:\n",
        "        simlist = [synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None]\n",
        "        if not simlist:\n",
        "            continue;\n",
        "        best_score = max(simlist)\n",
        " \n",
        "        # Check that the similarity could have been computed\n",
        "        score += best_score\n",
        "        count += 1\n",
        "\n",
        " \n",
        "    # Average the values\n",
        "    try:\n",
        "      score /= count\n",
        "    except Exception as e:\n",
        "      score = 0.0\n",
        "    return score\n",
        " \n",
        "sentences = [\n",
        "\n",
        "]\n",
        "\n",
        "focus_sentence = rank_res\n",
        "#link-node pair\n",
        "for a in label_master:\n",
        "  x=\"\"\n",
        "  x=a.replace(\"/\",\" or \").replace(\"  \",\" \")\n",
        "  sentences.append(x)\n",
        "CSVStudentAnswer = \" \".join(sentences)\n",
        "sum = 0\n",
        "j=0\n",
        "count = 0\n",
        "\n",
        "res1=[]\n",
        "for sentence in sentences:\n",
        "    sen1 = sentence.lower()\n",
        "    sim_res = sentence_similarity(sen1, focus_sentence)*100\n",
        "    res1.append(sim_res)\n",
        "    print (\"Similarity(\\\"%s\\\") = %s\" % (sentence, sim_res))\n",
        "    if sim_res > 50:\n",
        "      count = count+1\n",
        "    j=j+1\n",
        "\n",
        "bof_similarity_result= (count/j)*100\n",
        "bof_similarity_result=round(bof_similarity_result)\n",
        "print(bof_similarity_result)\n",
        "bof_res=\"\"\n",
        "if bof_similarity_result>75:\n",
        "  bof_res=\"Excellent\"\n",
        "elif bof_similarity_result>50:\n",
        "  bof_res=\"Good\"\n",
        "elif bof_similarity_result>25:\n",
        "  bof_res=\"Poor\"\n",
        "else:\n",
        "  bof_res=\"Failing\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUdzuyi9BqAO"
      },
      "source": [
        "a = {'Sentence': sentences, 'Wordnet similarity': res1}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(out_fil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6taoEuWGym"
      },
      "source": [
        "Embeddedness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGAOW6oEY18"
      },
      "source": [
        "embed_res=(l_len/(c_len-1))*100\n",
        "\n",
        "embed_res=round(embed_res)\n",
        "print(embed_res)\n",
        "embed_result =\"\"\n",
        "if embed_res>75:\n",
        "  embed_result = \"Excellent\"\n",
        "elif embed_res>50:\n",
        "  embed_result = \"Good\"\n",
        "elif embed_res>25:\n",
        "  embed_result = \"Poor\"\n",
        "else:\n",
        "  embed_result = \"Failing\"\n",
        "\n",
        "print(embed_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcLo28pGEjvC"
      },
      "source": [
        "res_link=[]\n",
        "res_link1=[]\n",
        "for i in link_master:\n",
        "  link_res = i.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"  \",\" \")\n",
        "  \n",
        "  res_link.append(link_res)\n",
        "\n",
        "all_link = len(res_link)\n",
        "\n",
        "uniq_link= len(Counter(res_link).keys())\n",
        "efflink_result=\"\"\n",
        "efflink= (uniq_link/all_link)*100\n",
        "efflink=round(efflink)\n",
        "if efflink == 100:\n",
        "  efflink_result=\"Excellent\"\n",
        "elif efflink > 75:\n",
        "  efflink_result=\"Good\"\n",
        "else:\n",
        "  cu = []\n",
        "  su=0\n",
        "  #can use this itself for efflink\n",
        "  for a in final_f:\n",
        "    cu.append(a)\n",
        "  res_effi =[]\n",
        "  for a in final_f:\n",
        "    j=\"\"\n",
        "    cu.remove(a)\n",
        "    j = \" \".join(cu)\n",
        "    resu = symmetric_sentence_similarity(a, j)*100\n",
        "    res_effi.append(resu)\n",
        "    cu.append(a)\n",
        "\n",
        "  for n in res_effi:\n",
        "    su = su + n\n",
        "\n",
        "  link_sim=su/len(res_effi)\n",
        "  if link_sim<50:\n",
        "    efflink_result=\"Poor\"\n",
        "  else:\n",
        "    efflink_result=\"Failing\"\n",
        "\n",
        "print(efflink_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeMtGRpEmrN"
      },
      "source": [
        "embed=collections.defaultdict(list)\n",
        "for i in final:\n",
        "  k=0\n",
        "  for j in final[i]:\n",
        "    if(k==0):\n",
        "      ans=j\n",
        "      ans=ans.replace(\"\\r\",\"\").replace(\"  \",\" \")\n",
        "\n",
        "    else:\n",
        "      j=j.replace(\"\\r\",\"\").replace(\"  \",\" \")\n",
        "      embed[ans].append(j)\n",
        "\n",
        "    k=k+1\n",
        "print (embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjHJymO-Ewt7"
      },
      "source": [
        "embed_link=collections.defaultdict(list)\n",
        "links=[]\n",
        "node_list=[]\n",
        "for i in label_master:\n",
        "  node_list.append(i)\n",
        "for i in link_master:\n",
        "  ans12=i.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
        "  links.append(ans12)\n",
        "for i in embed:\n",
        "  if i in links:\n",
        "    for j in embed[i]:\n",
        "      embed_link[i].append(j)\n",
        "\n",
        "for i in embed_link:\n",
        "  for j in embed_link[i]:\n",
        "    if j in node_list:\n",
        "      node_list.remove(j)\n",
        "    \n",
        "print (node_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKDyyxilEzVI"
      },
      "source": [
        "print(final)\n",
        "full=[]\n",
        "for i in final:\n",
        "  for j in final[i]:\n",
        "    aswe=j.replace(\"  \",\" \")\n",
        "    full.append(aswe)\n",
        "print(full)\n",
        "print(node_list)\n",
        "for i in node_list:\n",
        "  if i not in full:\n",
        "    node_list.remove(i)\n",
        "\n",
        "\n",
        "rootnode=node_list[0]\n",
        "rootnode=rootnode.replace(\"  \",\" \")\n",
        "print(node_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_-n2v4ZE5zj"
      },
      "source": [
        "print(rootnode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgGRagCi3LEC"
      },
      "source": [
        "#breadth if more than one rootnode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EwG526Z1lL"
      },
      "source": [
        "res_D = []\n",
        "if(len(node_list)>1):\n",
        "  for i in node_list:\n",
        "    rootnode = i\n",
        "    depth=0\n",
        "    dfs(visited, embed, rootnode, depth)\n",
        "    max_dep = 0\n",
        "    for i in depth_val:\n",
        "      if i > max_dep:\n",
        "        max_dep = i\n",
        "    result_depth=\"\"\n",
        "\n",
        "    depth_res=(max_dep-1)/2\n",
        "    depth_res=round(depth_res)\n",
        "    res_D.append(depth_res)\n",
        "    print(res_D)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ79YT9lbJ5L"
      },
      "source": [
        "depth_res=0\n",
        "for i in res_D:\n",
        "  if(i>depth_res):\n",
        "    depth_res = i\n",
        "print(depth_res)\n",
        "result_depth=sol_embed(depth_res)\n",
        "print(result_depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD4oejK33O62"
      },
      "source": [
        "#breadth if only one rootnode\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90DgHsAJI5GR"
      },
      "source": [
        "visited = set()\n",
        "depth_val=[]\n",
        "def dfs(visited, graph, node, depth):\n",
        "    if node not in visited:\n",
        "        visited.add(node)\n",
        "        depth=depth+1\n",
        "        depth_val.append(depth)\n",
        "        for neighbour in graph[node]:\n",
        "            dfs(visited, graph, neighbour, depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlh9cXVcI8BH"
      },
      "source": [
        "depth=0\n",
        "\n",
        "dfs(visited, embed, rootnode, depth)\n",
        "print(depth_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie9VVDwYI-fn"
      },
      "source": [
        "max_dep = 0\n",
        "for i in depth_val:\n",
        "  if i > max_dep:\n",
        "    max_dep = i\n",
        "result_depth=\"\"\n",
        "\n",
        "depth_res=(max_dep-1)/2\n",
        "depth_res=round(depth_res)\n",
        "if depth_res==0:\n",
        "  result_depth = \"Failing\"\n",
        "elif depth_res == 1:\n",
        "  result_depth = \"Poor\"\n",
        "elif depth_res == 2:\n",
        "  result_depth = \"Good\"\n",
        "else:\n",
        "  result_depth = \"Excellent\"\n",
        "print(result_depth)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkLVBb8pUiIQ"
      },
      "source": [
        "def check(check1):\n",
        "  if check1 == \"Excellent\" :\n",
        "    return 3\n",
        "  elif check1 == \"Good\":\n",
        "    return 2\n",
        "  elif check1 == \"Poor\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjEBX4pJVOuP"
      },
      "source": [
        "def sol(val):\n",
        "  if val == 3:\n",
        "    return \"Excellent\"\n",
        "  elif val == 2:\n",
        "    return \"Good\"\n",
        "  elif val == 1:\n",
        "    return \"Poor\"\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj6aSGaxgAyW"
      },
      "source": [
        "def sol_embed(val):\n",
        "  if val > 3:\n",
        "    return \"Excellent\"\n",
        "  elif val == 2:\n",
        "    return \"Good\"\n",
        "  elif val == 1:\n",
        "    return \"Poor\"\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2en6Z1BVLQa"
      },
      "source": [
        "bn1=check(bof_res)\n",
        "\n",
        "bn2=check(result_depth)\n",
        "\n",
        "che=round((bn1+bn2)/2)\n",
        "print(che)\n",
        "bn3=sol(che)\n",
        "print(bn3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywSgv3W_WxfH"
      },
      "source": [
        "efl1=check(efflink_result)\n",
        "efl2=check(sim_r)\n",
        "che2 = round((efl1+efl2)/2)\n",
        "\n",
        "efl3=sol(che2)\n",
        "print(efl3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz20be5sCWuP"
      },
      "source": [
        "#Exporting output to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzFbPcOmJBFO"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from csv import reader\n",
        "\n",
        "emb=[]\n",
        "ef=[]\n",
        "re=[]\n",
        "bo=[]\n",
        "si=[]\n",
        "bof=[]\n",
        "#Embed\n",
        "emb.append(embed_result)\n",
        "#efficient link\n",
        "ef.append(efl3)\n",
        "#descriptive link\n",
        "si.append(sim_r)\n",
        "#BON\n",
        "re.append(result_depth)\n",
        "bo.append(bof_res)\n",
        "bof.append(bn3)\n",
        "out_file = fil1.replace(\".cxl\",\"\")\n",
        "full_final = '/content/drive/My Drive/Auto_waterloo/res/try/' + out_file +'_AutoWaterloo.csv'\n",
        "a = {'Breadth of net': bof,'Embeddedness': emb, \"Use of descriptive links\":si,'Efficient links': ef, 'Breadth of net_depth': depth_res, \"Breadth of net_Nodes similarity ratio\":bof_similarity_result, 'Embed value':embed_res, 'Use of descriptive link':similarity_result,'efficient_link val':efflink}\n",
        "df3 = pd.DataFrame.from_dict(a).to_csv(full_final)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}